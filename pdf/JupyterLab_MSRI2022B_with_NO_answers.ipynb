{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smaller-exclusive",
   "metadata": {},
   "source": [
    "# Lab: Fisher Information Matrix and Profile Likehihood\n",
    "### Jessica Conrad, MSPH\n",
    "### MSRI Summer School on Algebraic Geometry July 2022 (Part 2)\n",
    "\n",
    "#### Based off of the Parameter Estimation Lab by Dr. Marisa Eisenberg found [here](https://epimath.org/epid-814-materials/Labs/EstimationLab/) and [here](https://epimath.org/epid-814-materials/Labs/IdentifiabilityUncertainty/IdentifiablilityUncertaintyLab.html)\n",
    "\n",
    "# Part 1: Recall our model\n",
    "\n",
    "Here we resume our exploration of the following model:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dS}{dt} & = \\mu -\\beta SI - \\mu S\\\\     \n",
    "\\frac{dI}{dt} & = \\beta SI - \\gamma I -\\mu I \\\\ \n",
    "\\frac{dR}{dt} & = \\gamma I - \\mu R,             \n",
    "\\end{align}\n",
    "$$\n",
    "with measurement equation $y = k I$. In this case we are using the reparameterized model where we use the following definition:\n",
    "* **S(t)** : susceptible fraction of the population at time *t*\n",
    "* **I(t)** : infected fraction of the population at time *t*\n",
    "* **R(t)** : removed fraction of the population at time *t*\n",
    "* __$\\beta$__ : infection rate; expected number of secondary infections per time *t*\n",
    "* __$\\gamma$__ : recovery rate; $\\frac{1}{\\gamma}$ is average time a person is infectious/infected\n",
    "* __$\\mu$__ : death rate\n",
    "\n",
    "We assumed the birth and death rates are slow enough to assume $\\mu = 0$. This assumption is only valid for fast acting diseases.\n",
    "As such, let's define our \"true\" parameter set such that $\\beta = 0.4$, $\\gamma = 0.25$, and $\\kappa = 80000.$\n",
    "\n",
    "Ok then we will provide for you the code we generated last session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "automated-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import the relevant libraries ####\n",
    "from math import *                         # useful math functions\n",
    "import numpy as np                         # useful array objects \n",
    "                                           # (also a core scientific computing library)\n",
    "\n",
    "from scipy.integrate import odeint as ode  # ode solver\n",
    "import matplotlib.pyplot as plt            # nice plotting commands, very similar to Matlab commands\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import norm\n",
    "import scipy.optimize as optimize          #optimizer function package\n",
    "\n",
    "#### Redefine the functions we wrote ####\n",
    "def model(ini, time_step, params):\n",
    "    #Define our ODE model\n",
    "    \n",
    "    ###########################\n",
    "    #Input:\n",
    "    #   ini           initial inputs for the state variables S, I, R\n",
    "    #   time_step     time step definition; used by the ODE wrapper\n",
    "    #   params        parameters for this model, in this case beta and gamma\n",
    "    #\n",
    "    #Output:\n",
    "    #   Y             vector of state variable equations for S, I, and R\n",
    "    ###########################\n",
    "\tY = np.zeros(3) #column vector for the state variables\n",
    "\tX = ini\n",
    "\tmu = 0\n",
    "\tbeta = params[0]\n",
    "\tgamma = params[1]\n",
    "\n",
    "\tY[0] = mu - beta*X[0]*X[1] - mu*X[0]         \n",
    "\tY[1] = beta*X[0]*X[1] - gamma*X[1] - mu*X[1] \n",
    "\tY[2] = gamma*X[1] - mu*X[2]                  \n",
    "\n",
    "\treturn Y\n",
    "\n",
    "\n",
    "def x0fcn(params, data):\n",
    "    #Set initial conditions\n",
    "    \n",
    "    ###########################\n",
    "    #Input:\n",
    "    #   data          true data to be fit\n",
    "    #   params        parameters for this mode, in this case beta, gamma, and kappainv\n",
    "    #\n",
    "    #Output:\n",
    "    #   X0            initial conditions for the SIR ODE          \n",
    "    ###########################\n",
    "\tS0 = 1.0 - (data[0]/params[2]) \n",
    "\tI0 = data[0]/params[2]         \n",
    "\tR0 = 0.0                       \n",
    "\tX0 = [S0, I0, R0]\n",
    "\n",
    "\treturn X0\n",
    "\n",
    "\n",
    "def yfcn(res, params):\n",
    "    #Define measurement equation\n",
    "    \n",
    "    ###########################\n",
    "    #Input:\n",
    "    #   res           simulated data results\n",
    "    #   params        parameters for this mode, in this case beta, gamma, and kappainv\n",
    "    #\n",
    "    #Output:\n",
    "    #   simulated reported data \n",
    "    ###########################\n",
    "\treturn res[:,1]*params[2]\n",
    "\n",
    "\n",
    "def NLL(params, data, times): \n",
    "    #Define the negative log likelihood\n",
    "    \n",
    "    ###########################\n",
    "    #Input:\n",
    "    #   params        parameters for this mode, in this case beta, gamma, and kappainv\n",
    "    #   data          true data to be fit\n",
    "    #   times         time points when the true data is recorded\n",
    "    #\n",
    "    #Output:\n",
    "    #   nll           negative log likelihood estimate    \n",
    "    ###########################\n",
    "\tparams = np.abs(params)\n",
    "\tdata = np.array(data)\n",
    "    \n",
    "    #Simulate the model with current parameters\n",
    "\tres = ode(model, x0fcn(params,data), times, args=(params,))\n",
    "    \n",
    "    #Apply the measurement equation\n",
    "\ty = yfcn(res, params)\n",
    "    \n",
    "    #Calculate the NLL for Poisson distribution\n",
    "\tnll = sum(y) - sum(data*np.log(y))   #(****remove for sandbox version of code****)\n",
    "    \n",
    "\t# note this is a slightly shortened version--there's an additive constant term missing but it \n",
    "\t# makes calculation faster and won't alter the threshold. Alternatively, can do:\n",
    "\t# nll = -sum(np.log(poisson.pmf(np.round(data),np.round(y)))) # the round is b/c Poisson is for (integer) count \n",
    "\t# data this can also barf if data and y are too far apart because the dpois will be ~0, which makes the log \n",
    "    # angry\n",
    "\t\n",
    "\t# ML using normally distributed measurement error (least squares)\n",
    "\t# nll = -sum(np.log(norm.pdf(data,y,0.1*np.mean(data)))) # example WLS assuming sigma = 0.1*mean(data)\n",
    "\t# nll = sum((y - data)**2)  # alternatively can do OLS but note this will mess with the thresholds \n",
    "\t#                             for the profile! This version of OLS is off by a scaling factor from\n",
    "\t#                             actual LL units.\n",
    "\treturn nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Data ####\n",
    "times = [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98]\n",
    "data = [97, 271, 860, 1995, 4419, 6549, 6321, 4763, 2571, 1385, 615, 302, 159, 72, 34]\n",
    "\n",
    "#### Define our parameter set ####\n",
    "params = [0.4, 0.25, 80000.0]        #make sure all the params and inition states are float\n",
    "paramnames = ['beta', 'gamma', 'k']\n",
    "\n",
    "#### Simulate the model ####\n",
    "ini = x0fcn(params,data)\n",
    "res = ode(model, ini, times, args=(params,))\n",
    "sim_measure = yfcn(res, params)\n",
    "\n",
    "#### Plot the data and simulation ####\n",
    "plt.plot(times, sim_measure, 'b-', linewidth=3, label='Model simulation')\n",
    "plt.plot(times, data, 'ko', linewidth=2, label='Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Individuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-loading",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Python Hint:</b> Why did I separate the coding blocks that contain <i>libraries and function definitions</i> versus <i> parameter settings and function calls</i>? Generally it is good coding practice to write your functions in a file separate from variables and parameters that you the user will change. This will help prevent user errors. We first call our libraries and functions, and only then do we run our code experiments. \n",
    "</div>\n",
    "\n",
    "Let's also take a moment to remember how the likelihood function `NLL` could be used to estimate our parameters by wrapping it in an optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Choose an optimizer and estimate parameter values ####\n",
    "optimizer = optimize.minimize(NLL, params, args=(data, times), method='Nelder-Mead')\n",
    "paramests = np.abs(optimizer.x)\n",
    "\n",
    "#### Re-generate initial case data based on new parameter estimates\n",
    "iniests = x0fcn(paramests, data)      \n",
    "\n",
    "#### Re-simulate and plot the model with the final parameter estimates ####\n",
    "#Simulate data\n",
    "xest = ode(model, iniests, times, args=(paramests,))    \n",
    "#Apply the measurement equation\n",
    "est_measure = yfcn(xest, paramests)                     \n",
    "\n",
    "#Plot\n",
    "plt.plot(times, est_measure, 'b-', linewidth=3, label='Model simulation')\n",
    "plt.plot(times, data, 'ko', linewidth=2, label='Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Individuals')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('The parameter estimates are beta = {params[0]:.4}, gamma = {params[1]:.4}, and kappa = {params[2]}'.format(params=paramests))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-nomination",
   "metadata": {},
   "source": [
    "# Part 2: Fisher Information Matrix\n",
    "\n",
    "Ok we are all caught up on where we left off. Let's set up a new function to calculate the Fisher Information Matrix. Recall from the lecture, the exact definition of the Fisher Information Matrix is:\n",
    "$$\n",
    "I(p)_{ij}= \\mathop{\\mathbb{E}}[(\\frac{\\partial}{\\partial p_i} log⁡ \\mathcal{L}(z,p))(\\frac{\\partial}{\\partial p_j}   log⁡ \\mathcal{L}(z,p))| p]\n",
    "$$\n",
    "\n",
    "How do we translate this into code? The expectation function is going to be dependent on your model and likelihood function $\\mathcal{L}(z,p)$, so we need to write code specific to the SIR model. We are not going to walk through this particular derivation and instead we will spend time learning how to make sense of the results.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The FIM is a statistical method dependent on the quality of your data $z$, where z is defined as $z_i = y_i + e_i$, where $y$ is the measurement equation and $e$ is the error in each measurement of that output. If you are unable to characterize or limit the noise in your ouput data $z$, then all you will get is nonsense. As the age old saying goes: Garbage in, garbage out.\n",
    "</div>\n",
    "\n",
    "### Here is the FIM function for the SIR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legitimate-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified FIM (Fisher information matirx) function for the SIR model\n",
    "# Marisa Eisenberg (marisae@umich.edu)\n",
    "# Yu-Han Kao (kaoyh@umich.edu) -7-9-17\n",
    "\n",
    "def minifisher (times, params, data, delta = 0.001):\n",
    "    #Calculate the FIM for the SIR model.\n",
    "    \n",
    "    ###########################\n",
    "    #Input:\n",
    "    #   times         time points when the true data is collected\n",
    "    #   params        parameters for this mode, in this case beta, gamma, and kappainv\n",
    "    #   data          true data to be fit\n",
    "    #   delta         fit parameter for FIM; preset to 0.001, but can be set by user\n",
    "    #\n",
    "    #Output:\n",
    "    #   simulated reported data \n",
    "    ###########################\n",
    "\t#params = np.array(params)\n",
    "\tlistX = []\n",
    "\tparams_1 = np.array (params)\n",
    "\tparams_2 = np.array (params)\n",
    "\tfor i in range(len(params)):\n",
    "\t\tparams_1[i] = params[i] * (1+delta)\n",
    "\t\tparams_2[i]= params[i] * (1-delta)\n",
    "\n",
    "\t\tres_1 = ode(model, x0fcn(params_1,data), times, args=(params_1,))\n",
    "\t\tres_2 = ode(model, x0fcn(params_2,data), times, args=(params_2,))\n",
    "\t\tsubX = (yfcn(res_1, params_1) - yfcn(res_2, params_2)) / (2 * delta * params[i])\n",
    "\t\tlistX.append(subX.tolist())\n",
    "\tX = np.matrix(listX)\n",
    "\tFIM = np.dot(X, X.transpose())\n",
    "\treturn FIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-pontiac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This FIM code is a numerical approximation method. Specifically, `MiniFisher` is using a numerical approximation for the derivative. Ariel Citr&oacute;n Arias's slides found <a href=\"http://www.nimbios.org/wordpress-training/parameter/wp-content/uploads/sites/14/2014/03/ols_sir_lecture.pdf\">here</a> give a nice introduction to estimation and sensitivity equations using the forward sensitivity equations instead. \n",
    "</div>\n",
    "\n",
    "Ok so now that we have our function, model, and data, let's try running this example and see what the FIM looks like and explore some of its features!\n",
    "\n",
    "### Write a line of code to generate the FIM for our model by looking at how the function call was defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "banned-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate the simplified Fisher Information Matrix (FIM) ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-bibliography",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Python Hint:</b> Are you wondering how to see what the objects you created actually look like? Try the `print` function. For example, after you generate the object called \"FIM\", in the next line write: `print(FIM)`. Click \"Run\" in the upper left corner and see what happens! \n",
    "</div>\n",
    "\n",
    "### Using `np.linalg.matrix_rank()`, calculate the rank of your new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baking-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate rank of FIM ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-electricity",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Let's Ponder:</b> What do we expect the structure of the FIM to look like? What does this tell us about the identifiability of the model? Try it out with the other un-scaled SIR model given in Lab (1) (or you can use the un-scaled SIR from the parameter estimation lab) -- how does that change things?\n",
    "</div>\n",
    "\n",
    "As a note to yourself, write down some of your findings and thoughts in the Markdown box below OR add new coding boxes to explore the suggested problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-religious",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "elect-august",
   "metadata": {},
   "source": [
    "# Part 3: Generating Profile Likelihoods\n",
    "\n",
    "The recovery rate $\\gamma$ is often approximately known, so let’s fix the value of $\\gamma=0.25$.\n",
    "Now we have only two unknown parameters, $\\beta$ and $\\kappa$.\n",
    "We want to plot the likelihood as a surface or heat map as a function of $\\beta$ and $\\kappa$ (i.e. so that color is the likelihood value, and your x and y axes are the $\\beta$ and $\\kappa$ values respectively. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Recall that $\\gamma$ is the recovery rate, and $\\frac{1}{\\gamma}$ is the time spent in state $I$, or \"time spent infected/infectious\". Generally, a person is considered \"infected\" in our model when they present symptoms of disease. Symptoms of disease are noticeable, so we can often accurately approximate the \"time spent infected/infectious\" -- the inverse value of $\\gamma$! Therefore, $\\gamma$ is often known within some bound of uncertainty.\n",
    "</div>\n",
    "\n",
    "As an example, here’s some code to plot the likelihood for the Poisson case we used earlier. First, choose your own $\\beta$ and $\\gamma$ range of value to explore and their interval.\n",
    "\n",
    "### Use `np.arange` to  define a sequence of values for $\\beta$ and $\\gamma$ that you want to itterate through. Use `np.zeros` to generate a blank matrix called \"likevals\" to store the likelihood values in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranges for each parameter, and make an empty matrix for the likelihood values\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "print(betarange)\n",
    "print(kapparange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-ultimate",
   "metadata": {},
   "source": [
    "I have sketched out some \"for\" loops for you. \n",
    "### How do we populate each value of the \"likevals\" matrix using the `NLL` functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each point on the contour plot and calculate the likelihood value at those coordinates\n",
    "for i in range(len(betarange)):\n",
    "    for j in range(len(kapparange)):\n",
    "        likevals[i,j] = ... #Recal NLL(params, data, times)\n",
    "        \n",
    "print(np.min(likevals))\n",
    "print(np.max(likevals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-thought",
   "metadata": {},
   "source": [
    "Ok now that we have the Liklihood values for different $\\beta$ and $\\gamma$ combinations, let's plot this matrix as a contour to visually assess what our results are. You can try different ranges for beta and kappa depending on how far out you want to look at the plot!\n",
    "\n",
    "### How does the shape of the likelihood change as you switch likelihood functions? \n",
    "It may not change much, but you can often notice small differences between likelihood choices. What does the likelihood landscape tell us about the parameter identifiability of this model, assuming $\\gamma$ is known?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make a contour plot! #####\n",
    "plt.contourf(betarange, kapparange, likevals)\n",
    "plt.xlabel('Beta Range')\n",
    "plt.ylabel('Kappa Range')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-bahamas",
   "metadata": {},
   "source": [
    "### Please write a discussion of your findings in the Markdown box that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-found",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "visible-retrieval",
   "metadata": {},
   "source": [
    "# Part 4: Profile Likelihood to Confidence Bounds on Parameters\n",
    "\n",
    "Now we have started to learn how a liklihood space might look if we fix one of our parameters and explore the combinations of other parameters. \n",
    "But in the lecture, we discussed looking at the likelihood plots of singular parameters when letting *all* parameters vary in value.\n",
    "\n",
    "In this section, we will explore <b>Profile Likelihood Plots</b> and <b>Likelihood-based Confidence Intervals</b>.\n",
    "\n",
    "To make our lives a little easier, I am not going to make you write the profile likelihood generator code. Instead, let's walk through the profile likelihood generator code and make sure that we have an understanding of what it does, and more importantly does what it claims to do! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "thirty-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Likelihood Generator\n",
    "# Marisa Eisenberg (marisae@umich.edu)\n",
    "# Yu-Han Kao (kaoyh@umich.edu) -7-9-17\n",
    "\n",
    "def profcost (fit_params, profparam, profindex, data, times, cost_func):\n",
    "\tparamstest = fit_params.tolist()\n",
    "\tparamstest.insert(profindex, profparam)\n",
    "\treturn cost_func (paramstest, data, times)\n",
    "\n",
    "\n",
    "# Input definitions\n",
    "# params = starting parameters (all, including the one to be profiled)\n",
    "# profparam = index within params for the parameter to be profiled\n",
    "#   ---reminder to make this allow you to pass the name instead later on\n",
    "# costfun = cost function for the model - should include params, times, and data as arguments.\n",
    "#   Note costfun doesn't need to be specially set up for fixing the profiled parameter, \n",
    "#   it's just the regular function you would use to estimate all the parameters\n",
    "#   (it will get reworked to fix one of them inside ProfLike)\n",
    "# times, data = data set (times & values, or whatever makes sense)\n",
    "#   ---possibly change this so it's included in costfun and not a separate set of inputs? Hmm.\n",
    "# perrange = the percent/fraction range to profile the parameter over (default is 0.5)\n",
    "# numpoints = number of points to profile at in each direction (default is 10)\n",
    "\n",
    "# Output\n",
    "# A list with:\n",
    "#   - profparvals: the values of the profiled parameter that were used\n",
    "#   - fnvals: the cost function value at each profiled parameter value\n",
    "#   - convergence: the convergence value at each profiled parameter value\n",
    "#   - paramestvals: the estimates of the other parameters at each profiled parameter value\n",
    "\n",
    "def proflike (params, profindex, cost_func, times, data, perrange = 0.5, numpoints = 10):\n",
    "\tprofrangedown = np.linspace(params[profindex], params[profindex] * (1 - perrange), numpoints).tolist()\n",
    "\tprofrangeup = np.linspace(params[profindex], params[profindex] * (1 + perrange), numpoints).tolist()[1:] #skip the duplicated values\n",
    "\tprofrange = [profrangedown, profrangeup]\n",
    "\tcurrvals = []\n",
    "\tcurrparams = []\n",
    "\tcurrflags = []\n",
    "\n",
    "\tfit_params = params.tolist() #make a copy of params so we won't change the origianl list\n",
    "\tfit_params.pop(profindex)\n",
    "\tprint('Starting profile...')\n",
    "\tfor i in range(len(profrange)):\n",
    "\t\tfor j in profrange[i]:\n",
    "\t\t\t#print(i, j)\n",
    "\t\t\toptimizer = optimize.minimize(profcost, fit_params, args=(j, profindex, data, times, cost_func), method='Nelder-Mead')\n",
    "\t\t\tfit_params = np.abs(optimizer.x).tolist() #save current fitted params as starting values for next round\n",
    "\t\t\t#print(optimizer.fun)\n",
    "\t\t\tcurrvals.append(optimizer.fun)\n",
    "\t\t\tcurrflags.append(optimizer.success)\n",
    "\t\t\tcurrparams.append(np.abs(optimizer.x).tolist())\n",
    "\n",
    "\t#structure the return output\n",
    "\tprofrangedown.reverse()\n",
    "\tout_profparam = profrangedown+profrangeup\n",
    "\ttemp_ind = range(len(profrangedown))\n",
    "\tout_params = [currparams[i] for i in reversed(temp_ind)]+currparams[len(profrangedown):]\n",
    "\tout_fvals = [currvals[i] for i in reversed(temp_ind)]+currvals[len(profrangedown):]\n",
    "\tout_flags = [currflags[i] for i in reversed(temp_ind)]+currflags[len(profrangedown):]\n",
    "\toutput = {'profparam': out_profparam, 'fitparam': np.array(out_params), 'fcnvals': out_fvals, 'convergence': out_flags}\n",
    "\treturn output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-entertainment",
   "metadata": {},
   "source": [
    "Hopefully you wrote yourself notes in the code above. Now let's start by defining our confidence interval.\n",
    "\n",
    "For the threshold to use in determining your confidence intervals, we note that $2(NLL(p)−NLL(\\hat{p}))$ (where NLL is the negative log likelihood) is approximately $\\chi^2$-distributed with degrees of freedom equal to the number of parameters fitted (including the profiled parameter). \n",
    "Then an approximate 95% (for example) confidence interval for $p$ can be made by taking all values of $p$ that lie within the 95$^{th}$ percentile range of the $\\chi^2$-distribution for the given degrees of freedom.\n",
    "\n",
    "In this case, for a 95% confidence interval, we have three total parameters we are estimating ($\\beta, \\gamma$, and $\\kappa$), so the $\\chi^2$ value for the 95$^{th}$ percentile is 7.8147. Then the confidence interval is any $p$ such that:\n",
    "$$\n",
    "NLL(p)\\leq NLL(\\hat{p})+7.8147/2\n",
    "$$\n",
    "\n",
    "In other words, our threshold is $NLL(\\hat{p})+7.8147/2=NLL(\\hat{p})+3.9074$, where $NLL(\\hat{p})$ is the cost function (aka likelihood function) value at our parameter estimates from Part (1).\n",
    "\n",
    "### Below we have provided code that automatically generates an appropriate confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "integral-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate profile likelihoods and confidence bounds ####\n",
    "import scipy.stats as stats\n",
    "\n",
    "threshold = stats.chi2.ppf(0.95,len(paramests))/2.0 + optimizer.fun\n",
    "perrange = 0.25 #percent range for profile to run across"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-bosnia",
   "metadata": {},
   "source": [
    "Now that we can determine if our parameter estimate is reasonable using our threshold, let's try profiling the parameters with a profile likelihood plot, as seen in our earlier lectures. \n",
    "\n",
    "### Are the parameters practically idenitifiable? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the individual parameter profiles ####\n",
    "\n",
    "profiles={}\n",
    "for i in range(len(paramests)):\n",
    "\tprofiles[paramnames[i]] = proflike(paramests, i, NLL, times, data, perrange=perrange)\n",
    "\tprint('Profiles have been generated.')\n",
    "\tplt.figure()\n",
    "\tplt.scatter(paramests[i], optimizer.fun, marker='*',label='True value', color='k',s=150, facecolors='w', edgecolors='k')\n",
    "\tplt.plot(profiles[paramnames[i]]['profparam'], profiles[paramnames[i]]['fcnvals'], 'k-', linewidth=2, label='Profile likelihood')\n",
    "\tplt.axhline(y=threshold, ls='--',linewidth=1.0, label='Threshold', color='k')\n",
    "\tplt.xlabel(paramnames[i])\n",
    "\tplt.ylabel('Negative log likelihood')\n",
    "\tplt.legend(scatterpoints = 1)\n",
    "\tparamnames_fit = [ n for n in paramnames if n not in [paramnames[i]]]\n",
    "\tparamests_fit = [v for v in paramests if v not in [paramests[i]]]\n",
    "\tprint(paramnames_fit)\n",
    "\tprint(paramests_fit)\n",
    "    \n",
    "#print(profiles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-vegetarian",
   "metadata": {},
   "source": [
    "[Double click here. Fill in this Markdown box with your answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the parameter relationships ####\n",
    "\n",
    "profiles={}\n",
    "for i in range(len(paramests)):\n",
    "\tprofiles[paramnames[i]] = proflike(paramests, i, NLL, times, data, perrange=perrange)\n",
    "\tprint('Profiles have been generated.')\n",
    "\tparamnames_fit = [ n for n in paramnames if n not in [paramnames[i]]]\n",
    "\tparamests_fit = [v for v in paramests if v not in [paramests[i]]]\n",
    "\tfor j in range(profiles[paramnames[i]]['fitparam'].shape[1]):\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(profiles[paramnames[i]]['profparam'],profiles[paramnames[i]]['fitparam'][:,j],'k-', linewidth=2, label=paramnames_fit[j])\n",
    "\t\tplt.scatter(paramests[i], paramests_fit[j], marker='*',label='True value', color='k',s=150, facecolors='w', edgecolors='k')\n",
    "\t\tplt.xlabel(paramnames[i])\n",
    "\t\tplt.ylabel(paramnames_fit[j])\n",
    "\t\tplt.legend(scatterpoints = 1)\n",
    "#print(profiles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-sandwich",
   "metadata": {},
   "source": [
    "What do we learn from the 2 parameter relationship analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-reply",
   "metadata": {},
   "source": [
    "# Part 5: Bonus! Back to that real life drama\n",
    "\n",
    "Lastly, let us consider the case where you are attempting to fit and forecast an ongoing epidemic (i.e. with incomplete data). Truncate your data to only include the first seven data points (i.e. just past the peak), then re-fit the model parameters and generate the profile likelihoods with the truncated data (you can also see if truncating the data affects the FIM rank!).\n",
    "\n",
    "* How do your parameter estimates change?\n",
    "\n",
    "* Does the practical identifiability of the parameters change? How so?\n",
    "\n",
    "* If any of the parameters were unidentifiable, examine the relationships between parameters that are generated in the profile likelihoods. Can you see any interesting relationships between parameters? What do you think might be going on—why has the identifiability changed?\n",
    "\n",
    "Please add coding and markdown blocks below this section as you explore and "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
